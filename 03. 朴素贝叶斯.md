# 贝叶斯决策
  - 贝叶斯决策论(Bayesian decision theory)是概率框架下实施决策的基本方法。
  - 对分类任务来说，在所有相关概率都已知的理想情形下，贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优的类别标记.
  - example:  
  假设有N种可能的类别标记，即 $Y=\{c_1,c_2,...,c_n\}$,$\lambda_{ij}$是将一个真实标识为$c_j$的样本分类为$c_i$所产生的损失，基于后验概率$P(c_i|x)$可获得将样本 x 分类为$c_i$所产生的期望损失(expection loss),即在样本x 上的条件风险(conditiona risk) $R(c_i|x)=\sum_{j=1}^{N}\lambda_{ij}P(c_j|x)$.  
  我们的任务是寻找一个判定准则h: X--> Y以最小化总体风险  
  R(h)=$E_x$[R(h(x)|x)]
  显然对每个样本x，若h能最小化条件风险R(h(x)|x),则总体风险也将被最小化。这就产生了贝叶斯判定准则：为最小化总体风险，只需在每个样本上选择那个能使条件风险R（c|x）最小的类别标记。  
  具体来说，如果目标是最小化分类错误率，则误判损失$\lambda_{ij}$可以写为是$\lambda_{ij}=if(i=j,0,1)$,此时条件风险R(c|x)=1 - P(c|x),于是最小化分类错误率的贝叶斯最优分类器为h(x)=arg maxP(c|x),即对每个样本x，选择能使后验概率P(c|x)最大的类别标记  
  因此对于贝叶斯分类器来说，后验概率P(c|x)尤其重要，但是在现实任务种通常难以直接获得，从这个角度来看机器学习所要实现的是基于优先的样本训练集尽可能准确的估计出后验概率。这个时候就用到了贝叶斯定理:
  $P(c|x)=\frac{P(x,c)}{P(x)}=\frac{P(c)P(x|c)}{P(x)}$  
  其中P(c)是先验概率；P(x|c)是样本x相对于类标记C 的条件概率，P(x)是用于归一化的因子。对于给定样本x，P(x)于类标记无关，因为估计P(c|x)的问题就转化为如何基于训练数据D来估计先验概率P(c)和似然P(x|c)
  对于其中的离散属性来说，假设概率密度函数服从正太分布，利用正太分布函数求P值
# 朴素贝叶斯分类器
  - 根据贝叶斯决策的公式不难发现，基于贝叶斯公式来估计后验概率的主要困难在于：类条件概率P(x|c)是所有属性上的联合概率，难以从优先的训练样本直接估计而得，为避免这个障碍，朴素贝叶斯假设属性之间互相独立。基于有限训练样本直接估计联合概率，在计算上将会遭遇组合爆炸问题，在数据上会遇到样本稀疏问题，属性值越多，问题越严重。
  基于属性条件独立性假设，
  $P(c|x)=\frac{P(x,c)}{P(x)}=\frac{P(c)P(x|c)}{P(x)}=\frac{P(c)}{P(x)}\prod_{i=1}^{d}P(x_i|c)$  
  其中d为属性数目,$x_i$为x在第i个属性上的取值。
  由于对于所有类别来说P(x)相同，因此基于上面贝叶斯判定准则有
  $h_{nb}(x)=arg maxP(c)\prod_{i=1}^{d}P(x_i|c)$  
  显然根据公式可以看到朴素贝叶斯分类器的训练过程就是基于训练集D来估计类鲜艳概率P(c),并为每个属性估计条件概率P($x_i$|c)  
  令$D_c$标识训练集D种第c类样本组成的集合，若有充足的独立通分布样本，则可以容易的估计出类先验概率$P(c)=\frac{|D_c|}{|D|}$  
  对离散属性而言，令$D_{c_ix_i}$表示$D_c$中在第i个属性上取值为$x_i$的样本组成的集合，则条件概率$P(x_i|c)$可估计为$p(x_i|c)=\frac{1}{\sqrt{2\pi}\sigma_{c,i}}exp^\left(-\frac{\left(x_i-\mu_{c,i}\right)^2}{2(\sigma_{c,i})^2}\right)$  
  - 需要注意的是，如果某个属性值在训练集中没有于某个类同时出现过，则直接用上式进行计算会导致概率连乘中出现0值，分类的结果都将为固定值，为了避免出现这种问题，即其他属性携带的信息被训练集中未出现的属性值抹去在估计概率值时通常要进行平滑处理，常用拉普拉斯修正。具体来说，令N表示训练集D中可能的类别数，$N_i$表示第i个属性可能的取值数，则先验概率更新为$P(c)=\frac{|D_c|+1}{|D|+N}$  
  类条件概率更新为$P(x_i|c)=\frac{|D_{c,x_i}|+1}{|D_c|+N_i}$
## 朴素贝叶斯算法原理
   - 1. 设$x={a_1,a_2,...,a_m}$为一个待分类项，而每个a为c的一个特征属性
   - 2. 有类别集合$Y={y_1,y_2,...,y_n}$
   - 3. 计算后验概率$P(y_1|x),P(y_2|x),P(y_3|x)...P(y_n|x)$
   - 4. 如果$P(y_k|x)=max{P(y_1|x),P(y_2|x),P(y_3|x)...P(y_n|x)}$,则$x\in{y_k}$
   - 贝叶斯的主要难点在于计算类条件概率，计算步骤如下：
     - a. 找到一个已经分类的待分类项合集，即训练样本集
     - b. 统计各类别下各个特征的条件概率估计，即  
          $P(a_1|y1),P(a_2|y1),...P(a_m|y1);P(a_1|y2),P(a_2|y2),...P(a_m|y2);...;P(a_1|yn),P(a_2|yn),...P(a_m|yn)$
     - c. 根据朴素贝叶斯的假设，各特征属性条件独立，根据贝叶斯定理求得后验概率如下  
          $P(y_i|x)=\frac{P(x|y_i)P(y_i)}{P(x)}$  
          对于所有类别来说P(x)为常数，因此最大化分子也可以认为最大化$P(x|y_i)$.在各特征属性独立的情况下上式可以继续拆分为  
          $P(y_i|x)=\frac{P(x|y_i)P(y_i)}{P(x)}=\frac{(P(a_1|y_i)P(a_2|y_i)...P(a_m|y_i))P(y_i)}{P(c)}=\frac{P(y_i)\prod_{j=1}^{m}P(a_j|y_i)}{P(x)}$  
          考虑到连乘容易造成溢出，可对公式进行log转化变为加法  
          $P(y_i|x)=\frac{P(x|y_i)P(y_i)}{P(x)}=\frac{(P(a_1|y_i)P(a_2|y_i)...P(a_m|y_i))P(y_i)}{P(x)}=\frac{P(y_i)\prod_{j=1}^{m}P(a_j|y_i)}{P(x)}=\frac{P(y_i)\sum_{j=1}^{m}\log(P(a_j|y_i))}{P(x)}$
## 朴素贝叶斯算法优缺点
   - 优点： 可以运用于大型数据库；方法简单，分类准确率高，速度快，所需额外参数少，对于缺失数据不敏感
   - 缺点： 假设各特征相互独立，显示情况往往很难达到；需要知道先验概率【对训练数据集要求很高】
