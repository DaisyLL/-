# 贝叶斯决策是什么
  - 贝叶斯决策论(Bayesian decision theory)是概率框架下实施决策的基本方法。
  - 对分类任务磊说，在所有相关概率都已知的理想情形下，贝叶斯决策轮考虑如何基于这些概率和误判损失来选择最优的类别标记.
  - example:  
  假设有N种可能的类别标记，即 $Y=\{c_1,c_2,...,c_n\}$,$\lambda_ij$是将一个真实标识为$c_j$的样本分类为$c_i$所产生的损失，基于后验概率$P(c_i|x)$可获得将样本 x 分类为$c_i$所产生的期望损失(expection loss),即在样本x 上的条件风险(conditiona risk) $R(c_i|x)=\sum_{j=1}^{N}\lambda_{ij}P(c_j|x)$.  
  我们的任务是寻找一个判定准则h: X--> Y以最小化总体风险  
  R(h)=$E_x$[R(h(x)|x)]
  显然对每个样本x，若h能最小化条件风险R(h(x)|x),则总体风险也将被最小化。这就产生了贝叶斯判定准则：为最小化总体风险，只需在每个样本上选择那个能使条件风险R（c|x）最小的类别标记。  
  具体来说，如果目标是最小化分类错误率，则误判损失$\lambda_{ij}$可以写为是$\lambda_{ij}=if(i=j,0,1)$,此时条件风险R(c|x)=1 - P(c|x),于是最小化分类错误率的贝叶斯最优分类器为h(x)=arg maxP(c|x),即对每个样本x，选择能使后验概率P(c|x)最大的类别标记  
  因此对于贝叶斯分类器来说，后验概率P(c|x)尤其重要，但是在现实任务种通常难以直接获得，从这个角度来看机器学习所要实现的是基于优先的样本训练集尽可能准确的估计出后验概率。这个时候就用到了贝叶斯定理:
  $P(c|x)=\frac{P(x,c)}{P(x)}=\frac{P(c)P(x|c)}{P(x)}$  
  其中P(c)是先验概率；P(x|c)是样本x相对于类标记C 的条件概率，P(x)是用于归一化的因子。对于给定样本x，P(x)于类标记无关，因为估计P(c|x)的问题就转化为如何基于训练数据D来估计先验概率P(c)和似然P(x|c)
  对于其中的离散属性来说，假设概率密度函数服从正太分布，利用正太分布函数求P值
  不难发现，基于贝叶斯公式来估计后验概率的主要困难在于：类条件概率P(x|c)是所有属性上的联合概率，难以从优先的训练样本直接估计而得，为避免这个障碍，朴素贝叶斯假设属性之间互相独立。基于有限训练样本直接估计联合概率，在计算上将会遭遇组合爆炸问题，在数据上会遇到样本稀疏问题，属性值越多，问题越严重。
  基于属性条件独立性假设，
  $P(c|x)=\frac{P(x,c)}{P(x)}=\frac{P(c)P(x|c)}{P(x)}=\frac{P(c)}{P(x)}\prod_{i=1}^{d}P(x_i|c)$  
  其中d为属性数目,$x_i$为x在第i个属性上的取值。
  由于对于所有类别来说P(x)相同，因此基于上面贝叶斯判定准则有
  $h_{nb}(x)=arg maxP(c)\prod_{i=1}^{d}P(x_i|c)$  
  显然根据公式可以看到朴素贝叶斯分类器的训练过程就是基于训练集D来估计类鲜艳概率P(c),并为每个属性估计条件概率P($x_i$|c)
  令$D_c$标识训练集D种第c类样本组成的集合，若有充足的独立通分布样本，则可以容易的估计出类先验概率$P(c)=\frac{|D_c|}{|D|}$
  对离散属性而言，令$D_{c_ix_i}$表示$D_c$中在第i个属性上取值为$x_i$的样本组成的集合，则条件概率$P(x_i|c)$可估计为$p(x_i|c)=\frac{1}{\sqrt({2\pi})\sigma_{c,i}}exp^\left(-\frac{\left(x_i-\mu_{c,x_i}\right)^2}{2(\sigma_{c,i})^2}\right)$
