# 决策树
  - 基本概念：
    - 分类决策数模型是一种描述对实例进行分类的树形结构。决策树由节点和有向边组成。结点有两种类型：内部结点和叶节点。
    - 内部节点表示一个特征或属性，叶节点表示一个类 
    - 决策树又称为判定树，是一种以树结构(包括二叉树和多叉树)形式来表达的预测分析模型
      - 通过把实例从根节点排列到某个叶节点来分类实例
      - 叶子节点即为实例所属的分类
      - 树上每个节点说明了对实例的某个属性的测试，节点的每个后继分支对应于该属性的一个可能值
  - 决策树结构
    - 根部结点--> 非叶子结点[代表测试的条件，对数据属性的测试] --> 分支[代表测试的结果] --> 叶结点[代表分类后所获得的分类标记]
  - 决策树种类
    - 分类决策树： 对离散变量做决策树
    - 回归树： 对连续变量做决策树
  - 决策树算法【贪心算法】
    - 有监督的学习
    - 非参数学习算法
    - 自顶向下递归方式构造决策树
    - 每一步选择中都采取在当前状态下最优的选择
    决策树学习的算法通常是一个递归的选择最优特征，并根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类的过程
    在决策树算法中，ID3基于信息增益作为最优特征的度量，C4.5基于信息增益比作为最优特征选择的度量，CART基于基尼指数作为属性选择的度量
  - 决策树学习过程
    - 特征选择
    - 决策树生成： 递归结构，对应于模型的局部最优
    - 决策树剪枝： 缩小树结构模型，缓解过拟合，对应于模型的全局最优
  - 决策树优缺点
    - 优点
      - 速度快： 计算量相对较小，容易转化为分类规则。只要沿着根节点一路直走到叶节点，沿途的分裂条件就能够唯一确定一条分类的谓词
      - 准确性高： 挖掘出的分类规则准确度比较高，便于理解，决策树可以清晰的显示哪些字段比较重要，即分类规则可理解
      - 可以处理连续和种类字段
      - 不需要任何领域知识和参数假设
      - 适合高纬度数据
    - 缺点
      - 易于过拟合
      - 对于各类别样本数量不一致的数据，信息增益偏向于具有更多数值的特征
      - 忽略属性之间的相关性
  - 涉及到的基本概念
    - 熵
      - 熵指的是事情的不确定程度，熵越大，不确定性越大。在数学上计算一件事情的熵，可以通过发生的概率值p$p_i$来计算
        
