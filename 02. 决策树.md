# 决策树
  - 基本概念：
    - 分类决策数模型是一种描述对实例进行分类的树形结构。决策树由节点和有向边组成。结点有两种类型：内部结点和叶节点。
    - 内部节点表示一个特征或属性，叶节点表示一个类 
    - 决策树又称为判定树，是一种以树结构(包括二叉树和多叉树)形式来表达的预测分析模型
      - 通过把实例从根节点排列到某个叶节点来分类实例
      - 叶子节点即为实例所属的分类
      - 树上每个节点说明了对实例的某个属性的测试，节点的每个后继分支对应于该属性的一个可能值
  - 决策树结构
    - 根部结点--> 非叶子结点[代表测试的条件，对数据属性的测试] --> 分支[代表测试的结果] --> 叶结点[代表分类后所获得的分类标记]
  - 决策树种类
    - 分类决策树： 对离散变量做决策树
    - 回归树： 对连续变量做决策树
  - 决策树算法【贪心算法】
    - 有监督的学习
    - 非参数学习算法
    - 自顶向下递归方式构造决策树
    - 每一步选择中都采取在当前状态下最优的选择
    决策树学习的算法通常是一个递归的选择最优特征，并根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类的过程
    在决策树算法中，ID3基于信息增益作为最优特征的度量，C4.5基于信息增益比作为最优特征选择的度量，CART基于基尼指数作为属性选择的度量
  - 决策树学习过程
    - 特征选择
    - 决策树生成： 递归结构，对应于模型的局部最优
    - 决策树剪枝： 缩小树结构模型，缓解过拟合，对应于模型的全局最优
  - 决策树优缺点
    - 优点
      - 速度快： 计算量相对较小，容易转化为分类规则。只要沿着根节点一路直走到叶节点，沿途的分裂条件就能够唯一确定一条分类的谓词
      - 准确性高： 挖掘出的分类规则准确度比较高，便于理解，决策树可以清晰的显示哪些字段比较重要，即分类规则可理解
      - 可以处理连续和种类字段
      - 不需要任何领域知识和参数假设
      - 适合高纬度数据
    - 缺点
      - 易于过拟合
      - 对于各类别样本数量不一致的数据，信息增益偏向于具有更多数值的特征
      - 忽略属性之间的相关性
  - 涉及到的基本概念
    - 熵
      - 熵指的是事情的不确定程度，熵越大，不确定性越大。在数学上计算一件事情的熵，可以通过发生的概率值$p_i$来计算
        $I = -(p1\log(p1) + p2\log(p2) + ... + pk\log(pk)) = -\sum_{i=1}^k\{p_i\log_2(p_i)}$
    - 条件熵： 类似于条件概念，度量事情在已知某个属性值的情况下的不确定性
      - $I = sum_{j=1}^n\p(y_j)I(X|y_i)$
# 算法
  - ID3
    - ID3算法利用信息增益来判断当前节点应该用什么特征来构建决策树，用计算的信息增益最大的特征来建立决策树的当前节点
    - 信息增益：$G(D,a) = I(D) - \sum_{v=1}^V\frac{|D^v|}{|D|}I(D^v)$
    - 举例说明。
      - 比如有15个样本D，输出为0或者1.其中有9个输出为1，6个输出为0.样本中有特征A，取值为A1,A2和A3。在取值为A1的样本输出中，有3个输出为1，2
        个输出为0，A2样本输出中2个输出为1，3个输出为0，A3样本输出中有4个输出为1，1个输出为0
        样本熵$I = -(\frac{9}{15}\log_2(\frac{9}{15}) + \frac{6}{15}\log_2(\frac{6}{15})) = 0.971$
        样本在特征A基础上的条件熵为 $I(D|A) = \frac{5}{15}I(D1) + \frac{5}{15}I(D2) + \frac{5}{15}I(D3)
                                          = \frac{5}{15}(-(frac{3}{5}\log_2(\frac{3}{5}) + frac{2}{5}\log_2(\frac{2}{5}))
                                            + \frac{5}{15}(-(frac{2}{5}\log_2(\frac{2}{5}) + frac{3}{5}\log_2(\frac{3}{5}))
                                            + \frac{5}{15}(-(frac{4}{5}\log_2(\frac{4}{5}) + frac{1}{5}\log_2(\frac{1}{5})) 
                                          = 0.888
        
