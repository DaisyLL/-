# 决策树
  - 基本概念：
    - 分类决策数模型是一种描述对实例进行分类的树形结构。决策树由节点和有向边组成。结点有两种类型：内部结点和叶节点。
    - 内部节点表示一个特征或属性，叶节点表示一个类 
    - 决策树又称为判定树，是一种以树结构(包括二叉树和多叉树)形式来表达的预测分析模型
      - 通过把实例从根节点排列到某个叶节点来分类实例
      - 叶子节点即为实例所属的分类
      - 树上每个节点说明了对实例的某个属性的测试，节点的每个后继分支对应于该属性的一个可能值
  - 决策树结构
    - 根部结点--> 非叶子结点[代表测试的条件，对数据属性的测试] --> 分支[代表测试的结果] --> 叶结点[代表分类后所获得的分类标记]
  - 决策树种类
    - 分类决策树： 对离散变量做决策树
    - 回归树： 对连续变量做决策树
  - 决策树算法【贪心算法】
    - 有监督的学习
    - 非参数学习算法
    - 自顶向下递归方式构造决策树
    - 每一步选择中都采取在当前状态下最优的选择
    决策树学习的算法通常是一个递归的选择最优特征，并根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类的过程
    在决策树算法中，ID3基于信息增益作为最优特征的度量，C4.5基于信息增益比作为最优特征选择的度量，CART基于基尼指数作为属性选择的度量
  - 决策树学习过程
    - 特征选择
    - 决策树生成： 递归结构，对应于模型的局部最优
    - 决策树剪枝： 缩小树结构模型，缓解过拟合，对应于模型的全局最优
  - 决策树优缺点
    - 优点
      - 速度快： 计算量相对较小，容易转化为分类规则。只要沿着根节点一路直走到叶节点，沿途的分裂条件就能够唯一确定一条分类的谓词
      - 准确性高： 挖掘出的分类规则准确度比较高，便于理解，决策树可以清晰的显示哪些字段比较重要，即分类规则可理解
      - 可以处理连续和种类字段
      - 不需要任何领域知识和参数假设
      - 适合高纬度数据
    - 缺点
      - 易于过拟合
      - 对于各类别样本数量不一致的数据，信息增益偏向于具有更多数值的特征
      - 忽略属性之间的相关性
  - 涉及到的基本概念
    - 熵
      - 熵指的是事情的不确定程度，熵越大，不确定性越大。在数学上计算一件事情的熵，可以通过发生的概率值$p_i$来计算
        $I = -(p1\log(p1) + p2\log(p2) + ... + pk\log(pk)) = -\sum_{i=1}^k\{p_i\log_2(p_i)}$
    - 条件熵： 类似于条件概念，度量事情在已知某个属性值的情况下的不确定性
      - $I = \sum_{j=1}^np(y_j)I(X|y_i)$
# 算法
  - ID3
    - ID3算法利用信息增益来判断当前节点应该用什么特征来构建决策树，用计算的信息增益最大的特征来建立决策树的当前节点
    - 信息增益：$G(D,a) = I(D) - \sum_{v=1}^V\frac{|D^v|}{|D|}I(D^v)$
    - 举例说明。
      - 比如有15个样本D，输出为0或者1.其中有9个输出为1，6个输出为0.样本中有特征A，取值为A1,A2和A3。在取值为A1的样本输出中，有3个输出为1，2
        个输出为0，A2样本输出中2个输出为1，3个输出为0，A3样本输出中有4个输出为1，1个输出为0
      - 样本熵$I = -(\frac{9}{15}\log_2(\frac{9}{15}) + \frac{6}{15}\log_2(\frac{6}{15})) = 0.971$
      - 样本在特征A基础上的条件熵 
      - $
          I(D|A) = \frac{5}{15}I(D1) + \frac{5}{15}I(D2) + \frac{5}{15}I(D3) \\\\
                 = \frac{5}{15}(-(\frac{3}{5}\log_2(\frac{3}{5}) + \frac{2}{5}\log_2(\frac{2}{5})) \\\\
                    + \frac{5}{15}(-(\frac{2}{5}\log_2(\frac{2}{5}) + \frac{3}{5}\log_2(\frac{3}{5})) \\\\
                    + \frac{5}{15}(-(\frac{4}{5}\log_2(\frac{4}{5}) + \frac{1}{5}\log_2(\frac{1}{5})) \\\\
                 = 0.888
        $
      - 信息增益为 $G(D,A) = I - I(D|A) = 0.083$
    - 算法中文过程
      - 初始化信息增益阈值$\epsilon$
      - 判断样本是否为同一类输出$D_i$,如果是则返回单节点数T，类别标记为$D_i$
      - 判断特征是否为空，如果是则返回单节点树T，标记类别为样本中输出类别D实例最多的类别
      - 计算A中各个特征（一共N个）对输出D的信息增益，选择信息增益最大的特征$A_j$
      - 如果$A_j$的信息增益小于阈值$\epsilon$,则返回单节点树T，标记类别为样本中输出类别D实例最多的类别
      - 否则，按照特征$A_j$的不同取值$A_ji$将对应的样本输出D分为不同的类别$D_i$。每个类别产生一个子节点，对应特征值为$A_ji$.返回增加了节点的T
      - 对于新的T，递归调用2-6步，得到子树$T_i$
    - 不足
      - 没有考虑如果特征值为连续型数值，无法使用ID3
      - ID3 使用信息增益大的特征优先建立决策树的节点。这样在相同条件下，特征值取值比较多的特征信息增益自然比特征值取值比较小的大
      - ID3算法对缺失值的情况没有做考虑
      - 没有考虑过拟合
   
   - python实现ID3
     ```python
     import pandas as pd
      from math import log
      import numpy as np
      import operator




      def getVotes(labels):
          classVotes = {}
          for i in labels:
              if i in classVotes:
                  classVotes[i] += 1
              else:
                  classVotes[i] = 1
          sortedVotes = sorted(classVotes.items(),key=operator.itemgetter(1),reverse=True)
          return sortedVotes[0][0]

      def caclulateEntorpy(data):
          numEntries = len(data)
          labelCounts = {}
          for featVec in data:
              currentLabel = featVec[-1]
              if currentLabel in labelCounts:
                  labelCounts[currentLabel] += 1
              else:
                  labelCounts[currentLabel] = 1
          entorpy = 0
          for key in labelCounts:
              prob = float(labelCounts[key])/numEntries
              entorpy -= prob * log(prob,2)
          return entorpy


      def getbestFeature(data):
          numFeature = len(data[0]) - 1
          baseEntorpy = caclulateEntorpy(data)
          bestInfoGain = 0.0
          bestFeature = -1
          for i in range(numFeature):
              featureList = data[:,i]
              uniqueVals = np.unique(featureList)
              newEntorpy = 0.0
              for value in uniqueVals:
                  subData = data[data[:,i]==value]
                  prob = len(subData)/float(len(data)) * caclulateEntorpy(subData)
                  newEntorpy += prob
              infoGain = baseEntorpy - newEntorpy
              if infoGain > bestInfoGain:
                  bestInfoGain = infoGain
                  bestFeature = i
          return bestFeature,data[:,bestFeature]

      def createTree(data,labels,data_full,labels_full):
          if data[:,-1].max() == data[:,-1].min():
              return data[:,-1][0]
          if len(data[0])== 1:
              return getVotes(data[:,-1])
          bestFeature,featVec = getbestFeature(data)
          bestFeatureLable = labels_full[bestFeature+1]
          myTree = {bestFeatureLable:{}}
          uniqueVals = np.unique(featVec)
          uniqueVals_all = np.unique(data_full[:,bestFeature]).tolist()
          labels = labels.drop(labels_full[bestFeature+1]) 
          for value in uniqueVals:
              uniqueVals_all.remove(value)
              sublabels = labels[:]
              subData = data[data[:,bestFeature]==value]
              myTree[bestFeatureLable][value] = createTree(subData,sublabels,data_full,labels_full)
          for value in uniqueVals_all:
              myTree[bestFeatureLable][value] = getVotes(data[:,-1])
          return myTree  

      pl.mpl.rcParams['font.sans-serif'] = ['SimHei']  ##中文可以正常展示
      decisionNode = dict(boxstyle='sawtooth',fc='0.8') #决策点样式
      leafNode = dict(boxstyle='round4',fc='0.8')
      arrow_args=dict(arrowstyle='<-')
      def getNumLeafs(myTree):
          numLeafs=0
          firstStr=list(myTree.keys())[0]
          secondDict=myTree[firstStr]
          for key in secondDict.keys():
              if type(secondDict[key]).__name__=='dict':
                  numLeafs+=getNumLeafs(secondDict[key])
              else: numLeafs+=1
          return numLeafs

      #计算树的最大深度
      def getTreeDepth(myTree):
          maxDepth=0
          firstStr=list(myTree.keys())[0]
          secondDict=myTree[firstStr]
          for key in secondDict.keys():
              if type(secondDict[key]).__name__=='dict':
                  thisDepth=1+getTreeDepth(secondDict[key])
              else: thisDepth=1
              if thisDepth>maxDepth:
                  maxDepth=thisDepth
          return maxDepth

      #画节点
      def plotNode(nodeTxt,centerPt,parentPt,nodeType):
          createPlot.ax1.annotate(nodeTxt,xy=parentPt,xycoords='axes fraction',\
          xytext=centerPt,textcoords='axes fraction',va="center", ha="center",\
          bbox=nodeType,arrowprops=arrow_args)

      #画箭头上的文字
      def plotMidText(cntrPt,parentPt,txtString):
          lens=len(txtString)
          xMid=(parentPt[0]+cntrPt[0])/2.0-lens*0.002
          yMid=(parentPt[1]+cntrPt[1])/2.0
          createPlot.ax1.text(xMid,yMid,txtString)

      def plotTree(myTree,parentPt,nodeTxt):
          numLeafs=getNumLeafs(myTree)
          depth=getTreeDepth(myTree)
          firstStr=list(myTree.keys())[0]
          cntrPt=(plotTree.x0ff+(1.0+float(numLeafs))/2.0/plotTree.totalW,plotTree.y0ff)
          plotMidText(cntrPt,parentPt,nodeTxt)
          plotNode(firstStr,cntrPt,parentPt,decisionNode)
          secondDict=myTree[firstStr]
          plotTree.y0ff=plotTree.y0ff-1.0/plotTree.totalD
          for key in secondDict.keys():
              if type(secondDict[key]).__name__=='dict':
                  plotTree(secondDict[key],cntrPt,str(key))
              else:
                  plotTree.x0ff=plotTree.x0ff+1.0/plotTree.totalW
                  plotNode(secondDict[key],(plotTree.x0ff,plotTree.y0ff),cntrPt,leafNode)
                  plotMidText((plotTree.x0ff,plotTree.y0ff),cntrPt,str(key))
          plotTree.y0ff=plotTree.y0ff+1.0/plotTree.totalD

      def createPlot(inTree):
          fig=plt.figure(1,facecolor='white')
          fig.clf()
          axprops=dict(xticks=[],yticks=[])
          createPlot.ax1=plt.subplot(111,frameon=False,**axprops)
          plotTree.totalW=float(getNumLeafs(inTree))
          plotTree.totalD=float(getTreeDepth(inTree))
          plotTree.x0ff=-0.5/plotTree.totalW
          plotTree.y0ff=1.0
          plotTree(inTree,(0.5,1.0),'')
          plt.show()

      df = pd.read_excel(r'D:\Users\lulib\Desktop\data1.xlsx',encoding='utf-8')
      data = df.values[:,1:]
      labels = df.columns
      myTree = createTree(data,labels,data,labels)
      print(myTree)
      createPlot(myTree)
      ```
    - python实现C4.5
      ```python
      import pandas as pd
      from math import log
      import numpy as np
      import operator
      import matplotlib.pyplot as plt
      import pylab as pl

      def getVote(data):
          classVotes = {}
          for i in data[:,-1]:
              if i in classVotes.keys():
                  classVotes[i] += 1
              else:
                  classVotes[i] = 1
          sortedVotes = sorted(classVotes.items(),key=operator.itemgetter(1),reverse=True)
          return sortedVotes[0][0]

      def calculateEntorpy(data):
          entorpy = 0.0
          tag = np.unique(data[:,-1])
          for i in range(len(tag)):
              subdata = data[data[:,-1]==tag[i]]
              entorpy -= len(subdata)/len(data) * log(len(subdata)/len(data))
          return entorpy

      def getBestFeature(data):
          baseEntorpy = calculateEntorpy(data)
          bestinfoGain = 0.0
          numFeature = len(data[0]) - 1
          bestFeature = -1
          bestFeaVec = np.unique(data[:,-1])
          datatype = 0
          for i in range(numFeature):
              if type(data[0][i]).__name__ == 'float':
                  vec = np.sort(data[:,i])
                  new_vec = [round((vec[i] + vec[i+1])/2,4) for i in range(len(vec)-1)]
                  infoGain = 0.0
                  vectype = 1
                  for value in new_vec:
                      subdata1 = data[data[:,i]<=value]
                      subdata2 = data[data[:,i]>value]
                      newEntropy = len(subdata1)/len(data) * calculateEntorpy(subdata1) + len(subdata2)/len(data) * calculateEntorpy(subdata2)
                      gain = baseEntorpy - newEntropy
                      if gain > infoGain:
                          infoGain = gain
                          featureVec = value           
              else:
                  featureVec = np.unique(data[:,i])
                  newEntropy = 0.0
                  vectype = 0
                  for value in featureVec:
                      subdata = data[data[:,i]==value]
                      newEntropy += len(subdata)/len(data) * calculateEntorpy(subdata)
                      infoGain = baseEntorpy - newEntropy
              if infoGain > bestinfoGain :
                  bestinfoGain = infoGain
                  bestFeature = i
                  bestFeaVec = featureVec
                  datatype = vectype
          return bestFeature,bestFeaVec,datatype


      def createTree(data,feature_list,data_all,feature_list_all):
          if len(np.unique(data[:,-1])) == 1:
              return data[0][-1]
          if len(feature_list) == 0:
              return getVote(data)
          bestFeature, bestFeaVec, datatype = getBestFeature(data)
          bestFeature_name = feature_list_all[bestFeature]
          feature_list = feature_list.drop(bestFeature_name)
          bestFV_all = np.unique(data_all[:,bestFeature]).tolist()
          myTree={bestFeature_name:{}}
          if datatype == 1:
              subdata1 = data[data[:,bestFeature]<=bestFeaVec]
              subdata2 = data[data[:,bestFeature]>bestFeaVec]
              myTree[bestFeature_name]['<=%f' %bestFeaVec] = createTree(subdata1,feature_list,data_all,feature_list_all)
              myTree[bestFeature_name]['>%f' %bestFeaVec] = createTree(subdata2,feature_list,data_all,feature_list_all)
          else:
              for value in bestFeaVec:
                  subdata = data[data[:,bestFeature]==value]
                  myTree[bestFeature_name][value] = createTree(subdata,feature_list,data_all,feature_list_all)
                  bestFV_all.remove(value)
              for value in bestFV_all:
                  myTree[bestFeature_name][value] = getVote(data)
          return myTree

      df = pd.read_excel(r'D:\Users\lulib\Desktop\data.xlsx',encoding='utf-8')
      data = df.values[:,1:]
      feature_list = df.columns[1:]
      myTree = createTree(data,feature_list,data,feature_list)
      print(myTree)
      ```
