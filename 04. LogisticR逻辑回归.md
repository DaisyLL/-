# 逻辑回归是什么
  - 面对一个回归或者分类问题，找到预测函数，建立代价函数，通过优化方法迭代求解出最优的预测函数参数，然后测试验证模型的精确度
  - 逻辑回归是一个分类模型，主要用于两分类问题【伯努利分布】，输出只有两种
  - 逻辑回归模型中，y是一个定性变量，比如y=1 或 0，logictic方法主要用于研究某些事件发生的概率
## 逻辑回归基本原理
  - 找到合适的预测函数hypothesis，一般表示为h函数，该函数是模型需要的分类函数，用来预测输入数据的判断结果。这个过程是非常关键的，需要对数据有一定的了解或分析，知道或者猜测预测函数的大概形式，线性函数或者非线性函数
  - 构造代价函数Cost函数，也可以称之为损失函数，该函数表示预测的输出 h 与训练数据类别 y 之间的偏差，可以是二者之间的差（h-y）或者是其他的形式。综合考虑所有训练函数的损失，求和或者平均cost函数，即为J(θ)函数，表示所有训练数据预测值于实际类别的偏差
  - 损失函数值越小代表预测函数准确度越高，所以这一步需要做的是找到损失函数的最小值，找函数的最小值方法比较多，logistic使用的是梯度下降法
## 逻辑回归具体过程
### 构造预测函数
  - 找到预测函数，函数输出值为两个类别，所以使用logistic函数，也称为sigmod函数，函数形式为：
                                    $g(z)=\frac{1}{1+e^{-z}}$  
                                    对应的函数图像是一个取值在0和1 之间的S型曲线
  - 接下来需要确定数据划分的边界类型，有一些需要线性边界，有一些需要非线性边界，这里我们只讨论线性边界的情况
  - 对于线性边界的情况，边界形式如下：$\theta_0 + \theta_1x_1 + ... + \\theta_nx_n=\sum_{i=0}^{n}\theta_ix_i=\theta^Tx$  
    构造预测函数为： $h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}$  
    $h_\theta(x)$函数的值有特殊含义，他表示结果取1的概率，因此对于输入x分类结果为类别1和类别0的概率分别为  
    $p(y=1|x;\theta) = h_\theta(x)$  
    $p(y=0|x;\theta) = 1 - h_\theta(x)$
### 构造cost函数
  - 最重要的问题就是如何定义损失函数
  - 对于而分类问题来说，预测函数默认结果为y=1的结果，因此，函数结果分布如下：  
  $$
  p(y|x)=\begin{cases}
  p,\quad y=1 \\\\
  1 - p,\quad y=0
  \end{cases}
  $$
  将上式整合到一起可以写为$p(y_i|x_i)=p^{y_i}(1-p)^{1-y_i}$
  解释一下这个公式就是说采集训练样本($x_i,y_i$),当$y_i=1$时，概率为p,反之则为1-p
  对于整体来说，只要把每个样本的概率相乘即可，公式表现为  
    
  $P_{all}= p(y_1|x_1)p(y_2|x_2)...p(y_n|x_n)= \prod_{i=1}^{n}p(y_i|x_i)= \prod_{i=1}^{n}p^{y_i}(1-p)^{1-y_i}$  
  
  连乘计算对于系统来说计算量比较大，可以对公式两边进行log计算转化为加法运算，即
  
  $\log{P_{all}}= \log{(\prod_{i=1}^{n}p^{y_i}(1-p)^{1-y_i})}= \sum_{i=1}^{n}(y_i\log{p_i}+(1-y_i)\log{(1-p_i)})$  
  
  因为函数结果概率默认为y=1的概率，因此需要概率值越大越好，1-上式就是损失的概率值即损失函数，损失函数写为：  
    
  $J(p)= 1 - \log{P_{all}} = 1 - \sum_{i=1}^{n}(y_i\log{p_i}+(1-y_i)\log{(1-p_i)})$
### 损失函数求解最优解
  - 在真实事件中并不能准确的知道概率是多少，只能观测到事件是否发生。只能知道已经发生的样本分类结果，需要通过不完整的样本结果估计影响概率P的参数，对于逻辑回归来说就是$\theta$值
  - 最大似然估计MLE就是一种估计参数的方法，样本的表现和总体总是相似的。因此在我们求解上式J(p)得最小的时候，即损失函数最小。我们就能够得到预估总体样本的参数
  - 因为损失函数为1-值，省略1- 取$J(p)=-\sum_{i=1}^{n}(y_i\log{p_i}+(1-y_i)\log{(1-p_i)})$。损失函数最小就是目标
#### J(P)是关于参数$\theta$的函数，可以对参数使用梯度下降来求解最优值
  - 梯度的定义  
    对于一个一维的标量x，有导数$x^{\'}$  
    对一个多维向量$x =(x_1,x_2,x_3,...,x_n)$来说，他的导数称为梯度，也就是分别对于它的每个分量求导数$x^{\'} =(x_1^{\'},x_2^{\'},x_3^{\'},...,x_n^{\'})$
  - 逻辑回归损失函数的导数为一个连续的凸函数，因为只会有一个全局最优的点，不存在局部最优，可以实现最优解
  - 接下来针对损失函数对每一个参数求导
  - $J(p)=-\sum_{i=1}^{n}(y_i\log{p_i}+(1-y_i)\log{(1-p_i)})$
   $$\begin{align}
       (J(p))^{\'}=(-\sum_{i=1}^{n}(y_i\log{p_i}+(1-y_i)\log{(1-p_i)}))^{\'}\\\\ 
                  =-\sum_{i=1}^{n}(y_i\frac{1}{p}p^{\'}+(1-p_i)\frac{1}{1-p_i}(1-p_i)^{\'})\\\\
    \end{align}
    $$  
  - 公式推导到这一步就变成了求导P的导数
 
   $$\begin{align}
    (p(\theta))^{\'}&= (\frac{1}{1+e^{\theta^Tx}})^{\'}\\\\
                    &= -\frac{1}{(1+e^{\theta^Tx})^2}e^{-\theta^Tx}(-\theta^Tx)^{\'}\\\\
                    &= -\frac{1}{1+e^{\theta^Tx}}\frac{e^{-\theta^Tx}}{1+e^{\theta^Tx}}(-\theta^Tx)^{\'}\\\\
                    &= -p(1-p)-x\\\\
                    &= p(1-p)x\\\\
   \end{align}
   $$
 
 - 已知P的导数可以求的J(p)关于$\theta$的导数可以写为
  $$\begin{align}
      (J(\theta))^{\'} & = (-\sum_{i=1}^{n}(y_i\log{p_i}+(1-y_i)\log{(1-p_i)}))^{\'}\\\\ 
                       & = -\sum_{i=1}^{n}(y_i\frac{1}{p}p^{\'}+(1-y_i)\frac{1}{1-p}(1-p)^{\'})\\\\
                       & = -\sum_{i=1}^{n}(y_i\frac{1}{p}p(1-p)x_i+(1-y_i)\frac{1}{1-p}(-p(1-p)x_i))\\\\
                       & = \sum_{i=1}^{n}(p-y_i)x_i\\\\
                       & = \sum_{i=1}^{n}(h_\theta(x_i)-y_i)x_i\\\\
    \end{align}
   $$
##### 梯度下降法
- 已知现在需要求得损失函数最小值对应的参数$\theta$,则根据梯度下降法可以得到$\theta$的更新过程：  
  $\alpha(J(\theta))^{\'}$
  $$begain{align}
   \theta^j: & =\theta^j - \alpha(J(\theta))^{\'}\\\\
             & =\theta^j - \alpha\sum_{i=1}^{n}(h_\theta(x_i)-y_i)x_i^j, (j=0...m)\\\\
  \end{align}
  $$
- 梯度下降过程向量化
