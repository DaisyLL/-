# 逻辑回归是什么
  - 面对一个回归或者分类问题，找到预测函数，建立代价函数，通过优化方法迭代求解出最优的预测函数参数，然后测试验证模型的精确度
  - 逻辑回归是一个分类模型，主要用于两分类问题【伯努利分布】，输出只有两种
  - 逻辑回归模型中，y是一个定性变量，比如y=1 或 0，logictic方法主要用于研究某些事件发生的概率
## 逻辑回归基本原理
  - 找到合适的预测函数hypothesis，一般表示为h函数，该函数是模型需要的分类函数，用来预测输入数据的判断结果。这个过程是非常关键的，需要对数据有一定的了解或分析，知道或者猜测预测函数的大概形式，线性函数或者非线性函数
  - 构造代价函数Cost函数，也可以称之为损失函数，该函数表示预测的输出 h 与训练数据类别 y 之间的偏差，可以是二者之间的差（h-y）或者是其他的形式。综合考虑所有训练函数的损失，求和或者平均cost函数，即为J(θ)函数，表示所有训练数据预测值于实际类别的偏差
  - 损失函数值越小代表预测函数准确度越高，所以这一步需要做的是找到损失函数的最小值，找函数的最小值方法比较多，logistic使用的是梯度下降法
## 逻辑回归具体过程
### 构造预测函数
  - 找到预测函数，函数输出值为两个类别，所以使用logistic函数，也称为sigmod函数，函数形式为：
                                    $g(z)=\frac{1}{1+e^{-z}}$  
                                    对应的函数图像是一个取值在0和1 之间的S型曲线
  - 接下来需要确定数据划分的边界类型，有一些需要线性边界，有一些需要非线性边界，这里我们只讨论线性边界的情况
  - 对于线性边界的情况，边界形式如下：$\theta_0 + \theta_1x_1 + ... + \\theta_nx_n=\sum_{i=0}^{n}\theta_ix_i=\theta^Tx$  
    构造预测函数为： $h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}$  
    $h_\theta(x)$函数的值有特殊含义，他表示结果取1的概率，因此对于输入x分类结果为类别1和类别0的概率分别为  
    $p(y=1|x;\theta) = h_\theta(x)$  
    $p(y=0|x;\theta) = 1 - h_\theta(x)$
### 构造cost函数
  - 最重要的问题就是如何定义损失函数
  - 对于而分类问题来说，预测函数默认结果为y=1的结果，因此，函数结果分布如下：  
  $$
  p(y|x)=\begin{cases}
  p,\quad y=1 \\\\
  1 - p,\quad y=0
  \end{cases}
  $$
  将上式整合到一起可以写为$p(y_i|x_i)=p^{y_i}(1-p)^{1-y_i}$
  解释一下这个公式就是说采集训练样本($x_i,y_i$),当$y_i=1$时，概率为p,反之则为1-p
  对于整体来说，只要把每个样本的概率相乘即可，公式表现为  
  $P_{all}=p(y_1|x_1)p(y_2|x_2)...p(y_n|x_n)  
        =\prod_{i=1}^{n}p(y_i|x_i)  
        =\prod_{i=1}^{n}p^{y_i}(1-p)^{1-y_i}$
