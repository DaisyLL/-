KNN 属于监督学习；
    监督学习就是给的样本都含有标签，分类的训练样本必须有标签，所以分类算法一般都是有监督算法。监督学习一般都是在规则话参数的同时最小化误差。最小化误差是为了结果更加准确，规则化参数是为了防止模型过拟合，提高泛化能力
    
1. 主要原理
   - 通过计算每个训练样本到待分类样本的距离，取和待分类样本距离最近的K个训练样例，K个样本中哪个类别的训练样本占多数，则待分类样本就属于哪个类别
   - 核心思想：
     - 如果一个样本在特征空间的K个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并且具有这个类别上样本的特性，该方法在确定分类决策上只依 
       据最近邻的一个或几个样本的类别来决定待分类样本的类别。KNN方法在类别决策时，只于及少量的相邻样本有关。由于KNN方法主要靠周围有限的临近的样本，而不
       是靠判别类域的方法确定所属类别，因此对于类域的交叉或重叠较多的待分类样本集来说，KNN比其它方法更适合
2. 算法描述
   - 算距离： 给定测试对象，计算它于训练集中的每个对象的距离[一般使用lp距离]
   - 对算到的距离数据排序，超过相似度阈值T则放入邻居案例集合NN
   - 自邻居案例集合NN中取出前K名，依多数决，得到样本类别
3. 算法步骤
   - 初始化距离为最大值
   - 计算未知样本和每个训练样本的距离 d
   - 得到目前K个最临近样本中的最大距离 m
   - 如果d 小于m,则该训练样本作为K-最近邻样本
   - 重复步骤2，3，4直到未知样本和所有训练样本的距离
   - 统计K最近邻样本中每个类标号出现的次数
   - 选择出现频率最大的类标号作为未知样本的类标号
   主要涉及3个因素： 训练集，相似度的衡量【即什么样算相似】，K 的大小
4. 算法优缺点
   - 优点
     - 简单，易于理解，实现方便，无需训练
     - 适合样本容量比较大的分类问题
     - 特别适合多分类问题
   - 缺点
     - 懒惰算法，计算量大，容易内存溢出
     - 可解释性差，只有结果，规则不能可视化
     - 可能会产生误分
5. 常见问题
   - K的取值问题
     - 太大近邻中含太多其它分类的点容易导致误分类【可以对距离加权，降低K值设定的影响】，泛化性能太差；太小分类容易受噪声点影响
     - k 值通常采用交叉检验来确定
     - 经验值： 通常使用低于训练样本数的平方根
   - 类别如何判定更合适
     - 单纯使用少数服从多数没有考虑距离的远近因素，距离更近的样本因该对结果产生更大的影响，可以使用加权法决定最终分类
   - 如何选择合适的距离度量
     - 高纬度对距离衡量的影响： 变量数越多，欧氏距离的区分能力就越差
     - 变量值域对距离的影响：最终的距离结果值可能会受距离绝对值的影响，因此先对维度变量进行标准化
   - 训练样本是否要一视同仁
     - 在训练集中，可以对某些样本进行加权，降低异常样本的影响
   - 性能问题
     - 因为要扫描全部训练样本算距离，因此内存消耗非常大
     - 可以压缩训练样本量等
6. 算法实现1
   ```python
    import numpy as np 
    import matplotlib.pyplot as plt
    import random
    import operator


    def loadData(filename):
        with open(filename,'r',encoding='utf-8') as file:
            r = file.read()
            data = [info.split(',') for info in r.strip().split('\n')]
            for x in range(len(data)):
                for y in range(len(data[0])):
                    data[x][y] = float(data[x][y])
            data = np.array(data)
            return data

    def show_scatter(data):
        for i in range(data.shape[1] -1):
            for j in range(data.shape[1]-1):
                if i > j:
                    fig = plt.figure()
                    ax = fig.add_subplot(111)
                    ax.scatter(data[:,i], data[:,j], c=data[:,-1])
                    plt.xlabel('feature %d' %i)
                    plt.ylabel('feature %d' %j)
                    plt.show()

    def train_test(data,precent,train=[],test=[]):
        for info in data:
            if random.random() < precent:
                train.append(info)
            else:
                test.append(info)
        train = np.array(train)
        test = np.array(test)
        return train,test

    def normlizeData(data):
        minVal = data.min(0)
        maxVal = data.max(0)
        ranges = maxVal - minVal

        normalData = (data - minVal) / ranges
        return normalData

    def data_distance(instance1,instance2,p):
        distance = 0
        for x in range(len(instance1)-1):
            distance += pow((instance1[x] - instance2[x]), p)
        return pow(distance,1/p)

    def getNeighbors(trainData,testInstance,k,p):
        distances = []
        neighbors=np.zeros((k,trainData.shape[1]))
        for x in range(len(trainData)):
            dist = data_distance(testInstance,trainData[x],p)
            distances.append((trainData[x],dist))
        distances.sort(key=operator.itemgetter(1))
        for x in range(k):
            neighbors[x]=distances[x][0]
        return neighbors 

    def getClassify(neighbors):
        classVotes = {}
        for x in range(len(neighbors)):
            response = neighbors[x][-1]
            # 如果neighbors是series格式由于series 本身有索引，所以如果其索引是整数索引的话，切片使用【-1】想要选取最后一个元素时，pandas会认为是对其本身索引的引用，由于其本身索引不存在-1，报错。
            # 但是如果索引不是整数类型而是其它类型时则-1索引取最后一个值就会生效。因为当series本身索引就是整数索引时，不要直接使用-1，
            # 可以使用位置索引iloc或者利用.values转化为ndarry再利用-1索引
            if response in classVotes:
                classVotes[response] += 1
            else:
                classVotes[response] = 1
        sortedVotes =  sorted(classVotes.items(),key=operator.itemgetter(1),reverse=True)
        return sortedVotes[0][0]
    def getAccuracy(testData,pred):
        correct = 0
        for i in range(len(testData)):
            if testData[i][-1] == pred[i]:
                correct += 1
        return round(correct/float(len(testData))*100,2)

    if __name__ == '__main__':
        filename = r'D:\Users\lulib\Desktop\iris.xlsx'
        k = 5
        p = 2
        precent = 0.67
        data = loadData(filename)
        show_scatter(data)
        nor_data = normlizeData(data)
        #train, test = train_test(data,precent,train=[],test=[])
        train, test = train_test(nor_data,precent,train=[],test=[])
        pred = np.zeros(test.shape[0])
        for x in range(len(test)):
            neighbors = getNeighbors(train,test[x],k,p)
            vote = getClassify(neighbors)
            pred[x]=vote
        accuarcy = getAccuracy(test,pred)
        hah = np.array([56594,8.9563,1.1934,3])
        hah_n = (hah-minVal)/ranges
        h_nei = getNeighbors(nor_data,hah_n,k,p)
        vote1 = getClassify(h_nei)
        print(accuarcy)
 ```
