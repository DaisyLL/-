KNN 属于监督学习；
    监督学习就是给的样本都含有标签，分类的训练样本必须有标签，所以分类算法一般都是有监督算法。监督学习一般都是在规则话参数的同时最小化误差。最小化误差是为了结果更加准确，规则化参数是为了防止模型过拟合，提高泛化能力
    
1. 主要原理
   - 通过计算每个训练样本到待分类样本的距离，取和待分类样本距离最近的K个训练样例，K个样本中哪个类别的训练样本占多数，则待分类样本就属于哪个类别
   - 核心思想：
     - 如果一个样本在特征空间的K个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并且具有这个类别上样本的特性，该方法在确定分类决策上只依 
       据最近邻的一个或几个样本的类别来决定待分类样本的类别。KNN方法在类别决策时，只于及少量的相邻样本有关。由于KNN方法主要靠周围有限的临近的样本，而不
       是靠判别类域的方法确定所属类别，因此对于类域的交叉或重叠较多的待分类样本集来说，KNN比其它方法更适合
2. 算法描述
   - 算距离： 给定测试对象，计算它于训练集中的每个对象的距离[一般使用lp距离]
   - 对算到的距离数据排序，超过相似度阈值T则放入邻居案例集合NN
   - 自邻居案例集合NN中取出前K名，依多数决，得到样本类别
3. 算法步骤
   - 初始化距离为最大值
   - 计算未知样本和每个训练样本的距离 d
   - 得到目前K个最临近样本中的最大距离 m
   - 如果d 小于m,则该训练样本作为K-最近邻样本
   - 重复步骤2，3，4直到未知样本和所有训练样本的距离
   - 统计K最近邻样本中每个类标号出现的次数
   - 选择出现频率最大的类标号作为未知样本的类标号
   主要涉及3个因素： 训练集，相似度的衡量【即什么样算相似】，K 的大小
4. 算法优缺点
   - 优点
     - 简单，易于理解，实现方便，无需训练
     - 适合样本容量比较大的分类问题
     - 特别适合多分类问题
   - 缺点
     - 懒惰算法，计算量大，容易内存溢出
     - 可解释性差，只有结果，规则不能可视化
     - 可能会产生误分
5. 常见问题
   - K的取值问题
     - 太大近邻中含太多其它分类的点容易导致误分类【可以对距离加权，降低K值设定的影响】，泛化性能太差；太小分类容易受噪声点影响
     - k 值通常采用交叉检验来确定
     - 经验值： 通常使用低于训练样本数的平方根
   - 类别如何判定更合适
     - 单纯使用少数服从多数没有考虑距离的远近因素，距离更近的样本因该对结果产生更大的影响，可以使用加权法决定最终分类
   - 如何选择合适的距离度量
     - 高纬度对距离衡量的影响： 变量数越多，欧氏距离的区分能力就越差
     - 变量值域对距离的影响：最终的距离结果值可能会受距离绝对值的影响，因此先对维度变量进行标准化
   - 训练样本是否要一视同仁
     - 在训练集中，可以对某些样本进行加权，降低异常样本的影响
   - 性能问题
     - 因为要扫描全部训练样本算距离，因此内存消耗非常大
     - 可以压缩训练样本量等
6. 算法实现
   ```python
   
